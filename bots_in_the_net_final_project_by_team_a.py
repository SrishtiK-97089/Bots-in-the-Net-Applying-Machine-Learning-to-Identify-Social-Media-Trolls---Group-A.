# -*- coding: utf-8 -*-
"""BOTS IN THE NET-FINAL PROJECT BY TEAM A

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WpMgBgUyuaeFf7aRtpxTzLIOgEF8Y8Lo
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import keras
import nltk
import unicodedata
from sklearn.feature_extraction.text import CountVectorizer
from keras.models import Model
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from tensorflow.keras.preprocessing.text import one_hot
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
from sklearn.utils import shuffle
import json
import random
import csv
import re
import pickle
from sklearn.metrics import accuracy_score
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.layers import Dropout
import tensorflow as tf
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.stem.porter import PorterStemmer
from bs4 import BeautifulSoup
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

f = open('/content/drive/MyDrive/Internship/Internship/tweets-2016-10000-textonly.txt', 'r')
lines = f.readlines()

tweets = []
labels = []

len_train = 1000

with open('/content/drive/MyDrive/Internship/Internship/IRAhandle_tweets_1.csv', newline='') as csvfile:
    categories = csvfile.readline().split(",")
    tweetreader = csv.reader(csvfile, delimiter=',')
    counter = 0
    for row in tweetreader:
        tweet = dict(zip(categories, row))
        if tweet['language'] == 'English':
            tweets.append(tweet['content'])
            labels.append(1)
            counter += 1
        if counter > len_train:
            break
csvfile.close()

# In[ ]:

for line in lines:
    # for line in lines:
    #     tweet = json.loads(line)
    #     if 'user' in tweet.keys():
    #         if tweet["user"]["lang"] == "en":
    #             tweets.append(tweet['text'])
    #             labels.append(0)
    tweets.append(line)
    labels.append(0)

f.close()
            
tweets_to_labels = dict(zip(tweets, labels))
random.shuffle(tweets)

actual = []

for tweet in tweets:
    actual.append(tweets_to_labels[tweet])
data=pd.DataFrame()
data['Text']=tweets
data['labels']=actual
data

tokenizer=ToktokTokenizer()
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

X=data.drop('labels',axis=1)
y=data['labels']
y.value_counts()

tweets=X.copy()
tweets.reset_index(inplace=False)
tweets

### Dataset Preprocessing
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
corpus = []
for i in range(0, len(tweets)):
    print(i)
    tweet = re.sub('[^a-zA-Z]', ' ', tweets['Text'][i])
    
    tweet = tweet.lower()
    tweet = tweet.split()
    
    tweet = [ps.stem(word) for word in tweet if not word in stopwords.words('english')]
    tweet = ' '.join(tweet)
    corpus.append(tweet)

corpus

# Creating the TF-IDF model
from sklearn.feature_extraction.text import TfidfVectorizer
cv = TfidfVectorizer()
X = cv.fit_transform(corpus).toarray()
y=data.iloc[:,1].values

### Create a Pickle file using serialization 
import pickle
pickle_out = open("vectorizer.pkl","wb")
pickle.dump(cv, pickle_out)
pickle_out.close()

# Train Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

# Training model using Naive bayes classifier

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB().fit(X_train, y_train)

from sklearn import metrics
y_pred=model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

pip install "scikit-learn==0.24.0"

# Create a Pickle file using serialization 
import pickle
pickle_out = open("classifier_NB.pkl","wb")
pickle.dump(model, pickle_out)
pickle_out.close()

# Training model using Logistic Regression
from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(X_train,y_train)

### Create a Pickle file using serialization 
import pickle
pickle_out = open("classifier_LR.pkl","wb")
pickle.dump(lr, pickle_out)
pickle_out.close()

y_pred=lr.predict(X_test)


print("Accuracy:",accuracy_score(y_test, y_pred))

# Training model using Random Forest classifier
from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier(random_state=1)
rfc.fit(X_train, y_train)

### Create a Pickle file using serialization 
import pickle
pickle_out = open("classifier_RF.pkl","wb")
pickle.dump(rfc, pickle_out)
pickle_out.close()

y_pred = rfc.predict(X_test)
print("Accuracy:",accuracy_score(y_test, y_pred))
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,y_pred)

# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(y_test, y_pred)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(y_test, y_pred)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(y_test, y_pred)
print('F1 score: %f' % f1)

# Training model using SVM
from sklearn import svm
clf = svm.SVC(kernel='rbf', gamma=2)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",accuracy_score(y_test, y_pred))

vocab_size=5000
one_hot_repr=[one_hot(words,vocab_size) for words in corpus]
one_hot_repr

sent_length=40
embedded_docs=pad_sequences(one_hot_repr,padding='pre',maxlen=sent_length)
embedded_docs

## Creating model
embedding_vector_features=80
model=Sequential()
model.add(Embedding(vocab_size,embedding_vector_features,input_length=sent_length))
model.add(LSTM(100))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())

import numpy as np
X_final=np.array(embedded_docs)
y_final=np.array(y)

from sklearn.model_selection import train_test_split
X_train1, X_test1, y_train1, y_test1 = train_test_split(X_final, y_final, test_size=0.2, random_state=42)

### Finally Training
model.fit(X_train1,y_train1,validation_data=(X_test1,y_test1),epochs=10,batch_size=64)

y_pred=model.predict_classes(X_test1)
y_pred

# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(y_test, y_pred)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(y_test, y_pred)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(y_test, y_pred)
print('F1 score: %f' % f1)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test1,y_pred)



#####Bidirectional LSTM
embedding_vector_features=80
model1=Sequential()
model1.add(Embedding(vocab_size,embedding_vector_features,input_length=sent_length))
model1.add(Bidirectional(LSTM(100)))
model1.add(Dropout(0.1))
model1.add(Dense(1,activation='sigmoid'))
model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model1.summary())

"""Bidirectional LSTM"""

### Finally training BIdirectional LSTM
model1.fit(X_train1,y_train1,validation_data=(X_test1,y_test1),epochs=10,batch_size=64)

from sklearn.metrics import accuracy_score
y_pred1=model1.predict_classes(X_test1)
y_pred1
accuracy_score(y_test1,y_pred1)

# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(y_test1, y_pred1)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(y_test1, y_pred1)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(y_test1, y_pred1)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(y_test1, y_pred1)
print('F1 score: %f' % f1)

model1.save("LSTM_TeamA_model.h5")
print("Saved model to disk")

"""TFIDF"""

import joblib
joblib.dump(model1, "data_transformer.joblib")













